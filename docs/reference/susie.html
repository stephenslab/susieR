<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Sum of Single Effects (SuSiE) Regression — susie • susieR</title><!-- katex math --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><script src="../katex-auto.js"></script><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.10/font.css" rel="stylesheet"><link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/Roboto_Slab-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Sum of Single Effects (SuSiE) Regression — susie"><meta name="description" content="Performs a sparse Bayesian multiple linear regression
of y on X, using the &quot;Sum of Single Effects&quot; model from Wang et al
(2020). In brief, this function fits the regression model \(y =
\mu + X b + e\), where elements of \(e\) are i.i.d. normal
with zero mean and variance residual_variance, \(\mu\) is
an intercept term and \(b\) is a vector of length p representing
the effects to be estimated. The &amp;#8220;susie assumption&amp;#8221; is that
\(b = \sum_{l=1}^L b_l\) where each \(b_l\) is a vector of
length p with exactly one non-zero element. The prior on the
non-zero element is normal with zero mean and variance var(y)
* scaled_prior_variance. The value of L is fixed, and
should be chosen to provide a reasonable upper bound on the number
of non-zero effects to be detected. Typically, the hyperparameters
residual_variance and scaled_prior_variance will be
estimated during model fitting, although they can also be fixed as
specified by the user. See functions susie_get_cs and
other functions of form susie_get_* to extract the most
commonly-used results from a susie fit.
#' @details The function susie implements the IBSS algorithm
from Wang et al (2020). The option refine = TRUE implements
an additional step to help reduce problems caused by convergence of
the IBSS algorithm to poor local optima (which is rare in our
experience, but can provide misleading results when it occurs). The
refinement step incurs additional computational expense that
increases with the number of CSs found in the initial run.
The function susie_ss implements essentially the same
algorithms, but using sufficient statistics. (The statistics are
sufficient for the regression coefficients \(b\), but not for the
intercept \(\mu\); see below for how the intercept is treated.)
If the sufficient statistics are computed correctly then the
results from susie_ss should be the same as (or very
similar to) susie, although runtimes will differ as
discussed below. The sufficient statistics are the sample
size n, and then the p by p matrix \(X'X\), the p-vector
\(X'y\), and the sum of squared y values \(y'y\), all computed
after centering the columns of \(X\) and the vector \(y\) to
have mean 0; these can be computed using compute_suff_stat.
The handling of the intercept term in susie_ss needs
some additional explanation. Computing the summary data after
centering X and y effectively ensures that the
resulting posterior quantities for \(b\) allow for an intercept
in the model; however, the actual value of the intercept cannot be
estimated from these centered data. To estimate the intercept term
the user must also provide the column means of \(X\) and the mean
of \(y\) (X_colmeans and y_mean). If these are not
provided, they are treated as NA, which results in the
intercept being NA. If for some reason you prefer to have
the intercept be 0 instead of NA then set
X_colmeans = 0,y_mean = 0.
For completeness, we note that if susie_ss is run on
\(X'X, X'y, y'y\) computed without centering \(X\) and
\(y\), and with X_colmeans = 0,y_mean = 0, this is
equivalent to susie applied to \(X, y\) with
intercept = FALSE (although results may differ due to
different initializations of residual_variance and
scaled_prior_variance). However, this usage is not
recommended for for most situations.
The computational complexity of susie is \(O(npL)\) per
iteration, whereas susie_ss is \(O(p^2L)\) per
iteration (not including the cost of computing the sufficient
statistics, which is dominated by the \(O(np^2)\) cost of
computing \(X'X\)). Because of the cost of computing \(X'X\),
susie will usually be faster. However, if \(n &amp;gt;&amp;gt; p\),
and/or if \(X'X\) is already computed, then
susie_ss may be faster."><meta property="og:description" content="Performs a sparse Bayesian multiple linear regression
of y on X, using the &quot;Sum of Single Effects&quot; model from Wang et al
(2020). In brief, this function fits the regression model \(y =
\mu + X b + e\), where elements of \(e\) are i.i.d. normal
with zero mean and variance residual_variance, \(\mu\) is
an intercept term and \(b\) is a vector of length p representing
the effects to be estimated. The &amp;#8220;susie assumption&amp;#8221; is that
\(b = \sum_{l=1}^L b_l\) where each \(b_l\) is a vector of
length p with exactly one non-zero element. The prior on the
non-zero element is normal with zero mean and variance var(y)
* scaled_prior_variance. The value of L is fixed, and
should be chosen to provide a reasonable upper bound on the number
of non-zero effects to be detected. Typically, the hyperparameters
residual_variance and scaled_prior_variance will be
estimated during model fitting, although they can also be fixed as
specified by the user. See functions susie_get_cs and
other functions of form susie_get_* to extract the most
commonly-used results from a susie fit.
#' @details The function susie implements the IBSS algorithm
from Wang et al (2020). The option refine = TRUE implements
an additional step to help reduce problems caused by convergence of
the IBSS algorithm to poor local optima (which is rare in our
experience, but can provide misleading results when it occurs). The
refinement step incurs additional computational expense that
increases with the number of CSs found in the initial run.
The function susie_ss implements essentially the same
algorithms, but using sufficient statistics. (The statistics are
sufficient for the regression coefficients \(b\), but not for the
intercept \(\mu\); see below for how the intercept is treated.)
If the sufficient statistics are computed correctly then the
results from susie_ss should be the same as (or very
similar to) susie, although runtimes will differ as
discussed below. The sufficient statistics are the sample
size n, and then the p by p matrix \(X'X\), the p-vector
\(X'y\), and the sum of squared y values \(y'y\), all computed
after centering the columns of \(X\) and the vector \(y\) to
have mean 0; these can be computed using compute_suff_stat.
The handling of the intercept term in susie_ss needs
some additional explanation. Computing the summary data after
centering X and y effectively ensures that the
resulting posterior quantities for \(b\) allow for an intercept
in the model; however, the actual value of the intercept cannot be
estimated from these centered data. To estimate the intercept term
the user must also provide the column means of \(X\) and the mean
of \(y\) (X_colmeans and y_mean). If these are not
provided, they are treated as NA, which results in the
intercept being NA. If for some reason you prefer to have
the intercept be 0 instead of NA then set
X_colmeans = 0,y_mean = 0.
For completeness, we note that if susie_ss is run on
\(X'X, X'y, y'y\) computed without centering \(X\) and
\(y\), and with X_colmeans = 0,y_mean = 0, this is
equivalent to susie applied to \(X, y\) with
intercept = FALSE (although results may differ due to
different initializations of residual_variance and
scaled_prior_variance). However, this usage is not
recommended for for most situations.
The computational complexity of susie is \(O(npL)\) per
iteration, whereas susie_ss is \(O(p^2L)\) per
iteration (not including the cost of computing the sufficient
statistics, which is dominated by the \(O(np^2)\) cost of
computing \(X'X\)). Because of the cost of computing \(X'X\),
susie will usually be faster. However, if \(n &amp;gt;&amp;gt; p\),
and/or if \(X'X\) is already computed, then
susie_ss may be faster."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">susieR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.14.43</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/mwe.html" aria-label="Get started with susieR"><span class="fa fa-play-circle"></span> Get Started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html" aria-label="Function reference"><span class="fa fa-file-code"></span> Functions</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/index.html" aria-label="Articles and vignettes"><span class="fa fa-book-reader"></span> Vignettes</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/announcements.html" aria-label="Package news"><span class="fa fa-newspaper"></span> News</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/StatFunGen/susieR" aria-label="View source on GitHub"><span class="fa fab fa-github"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch"><li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Sum of Single Effects (SuSiE) Regression</h1>
      <small class="dont-index">Source: <a href="https://github.com/stephenslab/susieR/blob/HEAD/R/susie.R" class="external-link"><code>R/susie.R</code></a></small>
      <div class="d-none name"><code>susie.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Performs a sparse Bayesian multiple linear regression
of y on X, using the "Sum of Single Effects" model from Wang et al
(2020). In brief, this function fits the regression model \(y =
\mu + X b + e\), where elements of \(e\) are <em>i.i.d.</em> normal
with zero mean and variance <code>residual_variance</code>, \(\mu\) is
an intercept term and \(b\) is a vector of length p representing
the effects to be estimated. The “susie assumption” is that
\(b = \sum_{l=1}^L b_l\) where each \(b_l\) is a vector of
length p with exactly one non-zero element. The prior on the
non-zero element is normal with zero mean and variance <code>var(y)
* scaled_prior_variance</code>. The value of <code>L</code> is fixed, and
should be chosen to provide a reasonable upper bound on the number
of non-zero effects to be detected. Typically, the hyperparameters
<code>residual_variance</code> and <code>scaled_prior_variance</code> will be
estimated during model fitting, although they can also be fixed as
specified by the user. See functions <code><a href="susie_get_methods.html">susie_get_cs</a></code> and
other functions of form <code>susie_get_*</code> to extract the most
commonly-used results from a susie fit.</p>
<p>#' @details The function <code>susie</code> implements the IBSS algorithm
from Wang et al (2020). The option <code>refine = TRUE</code> implements
an additional step to help reduce problems caused by convergence of
the IBSS algorithm to poor local optima (which is rare in our
experience, but can provide misleading results when it occurs). The
refinement step incurs additional computational expense that
increases with the number of CSs found in the initial run.</p>
<p>The function <code>susie_ss</code> implements essentially the same
algorithms, but using sufficient statistics. (The statistics are
sufficient for the regression coefficients \(b\), but not for the
intercept \(\mu\); see below for how the intercept is treated.)
If the sufficient statistics are computed correctly then the
results from <code>susie_ss</code> should be the same as (or very
similar to) <code>susie</code>, although runtimes will differ as
discussed below. The sufficient statistics are the sample
size <code>n</code>, and then the p by p matrix \(X'X\), the p-vector
\(X'y\), and the sum of squared y values \(y'y\), all computed
after centering the columns of \(X\) and the vector \(y\) to
have mean 0; these can be computed using <code>compute_suff_stat</code>.</p>
<p>The handling of the intercept term in <code>susie_ss</code> needs
some additional explanation. Computing the summary data after
centering <code>X</code> and <code>y</code> effectively ensures that the
resulting posterior quantities for \(b\) allow for an intercept
in the model; however, the actual value of the intercept cannot be
estimated from these centered data. To estimate the intercept term
the user must also provide the column means of \(X\) and the mean
of \(y\) (<code>X_colmeans</code> and <code>y_mean</code>). If these are not
provided, they are treated as <code>NA</code>, which results in the
intercept being <code>NA</code>. If for some reason you prefer to have
the intercept be 0 instead of <code>NA</code> then set
<code>X_colmeans = 0,y_mean = 0</code>.</p>
<p>For completeness, we note that if <code>susie_ss</code> is run on
\(X'X, X'y, y'y\) computed <em>without</em> centering \(X\) and
\(y\), and with <code>X_colmeans = 0,y_mean = 0</code>, this is
equivalent to <code>susie</code> applied to \(X, y\) with
<code>intercept = FALSE</code> (although results may differ due to
different initializations of <code>residual_variance</code> and
<code>scaled_prior_variance</code>). However, this usage is not
recommended for for most situations.</p>
<p>The computational complexity of <code>susie</code> is \(O(npL)\) per
iteration, whereas <code>susie_ss</code> is \(O(p^2L)\) per
iteration (not including the cost of computing the sufficient
statistics, which is dominated by the \(O(np^2)\) cost of
computing \(X'X\)). Because of the cost of computing \(X'X\),
<code>susie</code> will usually be faster. However, if \(n &gt;&gt; p\),
and/or if \(X'X\) is already computed, then
<code>susie_ss</code> may be faster.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">susie</span><span class="op">(</span></span>
<span>  <span class="va">X</span>,</span>
<span>  <span class="va">y</span>,</span>
<span>  L <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  scaled_prior_variance <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  residual_variance <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  prior_weights <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  null_weight <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  standardize <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  intercept <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  estimate_residual_variance <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  estimate_residual_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"MoM"</span>, <span class="st">"MLE"</span>, <span class="st">"Servin_Stephens"</span><span class="op">)</span>,</span>
<span>  estimate_prior_variance <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  estimate_prior_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"optim"</span>, <span class="st">"EM"</span>, <span class="st">"simple"</span><span class="op">)</span>,</span>
<span>  unmappable_effects <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"none"</span>, <span class="st">"inf"</span>, <span class="st">"ash"</span><span class="op">)</span>,</span>
<span>  check_null_threshold <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  prior_tol <span class="op">=</span> <span class="fl">1e-09</span>,</span>
<span>  residual_variance_upperbound <span class="op">=</span> <span class="cn">Inf</span>,</span>
<span>  model_init <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  coverage <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>  min_abs_corr <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  compute_univariate_zscore <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  na.rm <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  max_iter <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  tol <span class="op">=</span> <span class="fl">0.001</span>,</span>
<span>  convergence_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"elbo"</span>, <span class="st">"pip"</span><span class="op">)</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  track_fit <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  residual_variance_lowerbound <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">var</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/drop.html" class="external-link">drop</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="fl">10000</span>,</span>
<span>  refine <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  n_purity <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  alpha0 <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  beta0 <span class="op">=</span> <span class="fl">0</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-x">X<a class="anchor" aria-label="anchor" href="#arg-x"></a></dt>
<dd><p>An n by p matrix of covariates.</p></dd>


<dt id="arg-y">y<a class="anchor" aria-label="anchor" href="#arg-y"></a></dt>
<dd><p>The observed responses, a vector of length n.</p></dd>


<dt id="arg-l">L<a class="anchor" aria-label="anchor" href="#arg-l"></a></dt>
<dd><p>Maximum number of non-zero effects in the model. If L is larger than
the number of covariates, p, L is set to p.</p></dd>


<dt id="arg-scaled-prior-variance">scaled_prior_variance<a class="anchor" aria-label="anchor" href="#arg-scaled-prior-variance"></a></dt>
<dd><p>The prior variance, divided by
<code>var(y)</code> (or by <code>(1/(n-1))yty</code> for
<code>susie_ss</code>); that is, the prior variance of each
non-zero element of b is <code>var(y) * scaled_prior_variance</code>. The
value provided should be either a scalar or a vector of length
<code>L</code>. If <code>estimate_prior_variance = TRUE</code>, this provides
initial estimates of the prior variances.</p></dd>


<dt id="arg-residual-variance">residual_variance<a class="anchor" aria-label="anchor" href="#arg-residual-variance"></a></dt>
<dd><p>Variance of the residual. If
<code>estimate_residual_variance = TRUE</code>, this value provides the
initial estimate of the residual variance. By default, it is set to
<code>var(y)</code> in <code>susie</code> and <code>(1/(n-1))yty</code> in
<code>susie_ss</code>.</p></dd>


<dt id="arg-prior-weights">prior_weights<a class="anchor" aria-label="anchor" href="#arg-prior-weights"></a></dt>
<dd><p>A vector of length p, in which each entry
gives the prior probability that corresponding column of X has a
nonzero effect on the outcome, y.</p></dd>


<dt id="arg-null-weight">null_weight<a class="anchor" aria-label="anchor" href="#arg-null-weight"></a></dt>
<dd><p>Prior probability of no effect (a number between 0 and 1,
and cannot be exactly 1).</p></dd>


<dt id="arg-standardize">standardize<a class="anchor" aria-label="anchor" href="#arg-standardize"></a></dt>
<dd><p>If <code>standardize = TRUE</code>, standardize the
columns of X to unit variance prior to fitting (or equivalently
standardize XtX and Xty to have the same effect). Note that
<code>scaled_prior_variance</code> specifies the prior on the
coefficients of X <em>after</em> standardization (if it is
performed). If you do not standardize, you may need to think more
carefully about specifying <code>scaled_prior_variance</code>. Whatever
your choice, the coefficients returned by <code>coef</code> are given for
<code>X</code> on the original input scale. Any column of <code>X</code> that
has zero variance is not standardized.</p></dd>


<dt id="arg-intercept">intercept<a class="anchor" aria-label="anchor" href="#arg-intercept"></a></dt>
<dd><p>If <code>intercept = TRUE</code>, the intercept is
fitted; it <code>intercept = FALSE</code>, the intercept is set to
zero. Setting <code>intercept = FALSE</code> is generally not
recommended.</p></dd>


<dt id="arg-estimate-residual-variance">estimate_residual_variance<a class="anchor" aria-label="anchor" href="#arg-estimate-residual-variance"></a></dt>
<dd><p>If
<code>estimate_residual_variance = TRUE</code>, the residual variance is
estimated, using <code>residual_variance</code> as an initial value. If
<code>estimate_residual_variance = FALSE</code>, the residual variance is
fixed to the value supplied by <code>residual_variance</code>.</p></dd>


<dt id="arg-estimate-residual-method">estimate_residual_method<a class="anchor" aria-label="anchor" href="#arg-estimate-residual-method"></a></dt>
<dd><p>The method used for estimating residual variance.
For the original SuSiE model, "MLE" and "MoM" estimation is equivalent, but for
the infinitesimal model, "MoM" is more stable. We recommend using "Servin_Stephens"
when n &lt; 80 for improved coverage, although it is currently only implemented
for individual-level data.</p></dd>


<dt id="arg-estimate-prior-variance">estimate_prior_variance<a class="anchor" aria-label="anchor" href="#arg-estimate-prior-variance"></a></dt>
<dd><p>If <code>estimate_prior_variance =
TRUE</code>, the prior variance is estimated (this is a separate
parameter for each of the L effects). If provided,
<code>scaled_prior_variance</code> is then used as an initial value for
the optimization. When <code>estimate_prior_variance = FALSE</code>, the
prior variance for each of the L effects is determined by the
value supplied to <code>scaled_prior_variance</code>.</p></dd>


<dt id="arg-estimate-prior-method">estimate_prior_method<a class="anchor" aria-label="anchor" href="#arg-estimate-prior-method"></a></dt>
<dd><p>The method used for estimating prior
variance. When <code>estimate_prior_method = "simple"</code> is used, the
likelihood at the specified prior variance is compared to the
likelihood at a variance of zero, and the setting with the larger
likelihood is retained.</p></dd>


<dt id="arg-unmappable-effects">unmappable_effects<a class="anchor" aria-label="anchor" href="#arg-unmappable-effects"></a></dt>
<dd><p>The method for modeling unmappable effects:
"none", "inf", "ash".</p></dd>


<dt id="arg-check-null-threshold">check_null_threshold<a class="anchor" aria-label="anchor" href="#arg-check-null-threshold"></a></dt>
<dd><p>When the prior variance is estimated,
compare the estimate with the null, and set the prior variance to
zero unless the log-likelihood using the estimate is larger by this
threshold amount. For example, if you set
<code>check_null_threshold = 0.1</code>, this will "nudge" the estimate
towards zero when the difference in log-likelihoods is small. A
note of caution that setting this to a value greater than zero may
lead the IBSS fitting procedure to occasionally decrease the ELBO. This
setting is disabled when using <code>unmappable_effects = "inf"</code> or
<code>unmappable_effects = "ash"</code>.</p></dd>


<dt id="arg-prior-tol">prior_tol<a class="anchor" aria-label="anchor" href="#arg-prior-tol"></a></dt>
<dd><p>When the prior variance is estimated, compare the
estimated value to <code>prior_tol</code> at the end of the computation,
and exclude a single effect from PIP computation if the estimated
prior variance is smaller than this tolerance value.</p></dd>


<dt id="arg-residual-variance-upperbound">residual_variance_upperbound<a class="anchor" aria-label="anchor" href="#arg-residual-variance-upperbound"></a></dt>
<dd><p>Upper limit on the estimated
residual variance. It is only relevant when
<code>estimate_residual_variance = TRUE</code>.</p></dd>


<dt id="arg-model-init">model_init<a class="anchor" aria-label="anchor" href="#arg-model-init"></a></dt>
<dd><p>A previous susie fit with which to initialize.</p></dd>


<dt id="arg-coverage">coverage<a class="anchor" aria-label="anchor" href="#arg-coverage"></a></dt>
<dd><p>A number between 0 and 1 specifying the
“coverage” of the estimated confidence sets.</p></dd>


<dt id="arg-min-abs-corr">min_abs_corr<a class="anchor" aria-label="anchor" href="#arg-min-abs-corr"></a></dt>
<dd><p>Minimum absolute correlation allowed in a
credible set. The default, 0.5, corresponds to a squared
correlation of 0.25, which is a commonly used threshold for
genotype data in genetic studies.</p></dd>


<dt id="arg-compute-univariate-zscore">compute_univariate_zscore<a class="anchor" aria-label="anchor" href="#arg-compute-univariate-zscore"></a></dt>
<dd><p>If <code>compute_univariate_zscore
= TRUE</code>, the univariate regression z-scores are outputted for each
variable.</p></dd>


<dt id="arg-na-rm">na.rm<a class="anchor" aria-label="anchor" href="#arg-na-rm"></a></dt>
<dd><p>Drop any missing values in y from both X and y.</p></dd>


<dt id="arg-max-iter">max_iter<a class="anchor" aria-label="anchor" href="#arg-max-iter"></a></dt>
<dd><p>Maximum number of IBSS iterations to perform.</p></dd>


<dt id="arg-tol">tol<a class="anchor" aria-label="anchor" href="#arg-tol"></a></dt>
<dd><p>tol A small, non-negative number specifying the convergence
tolerance for the IBSS fitting procedure.</p></dd>


<dt id="arg-convergence-method">convergence_method<a class="anchor" aria-label="anchor" href="#arg-convergence-method"></a></dt>
<dd><p>When <code>converge_method = "elbo"</code> the fitting
procedure halts when the difference in the variational lower bound, or
“ELBO” (the objective function to be maximized), is
less than <code>tol</code>. When <code>converge_method = "pip"</code> the fitting
procedure halts when the maximum absolute difference in <code>alpha</code> is less
than <code>tol</code>.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>If <code>verbose = TRUE</code>, the algorithm's progress,
a summary of the optimization settings, and refinement progress (if
<code>refine = TRUE</code>) are printed to the console.</p></dd>


<dt id="arg-track-fit">track_fit<a class="anchor" aria-label="anchor" href="#arg-track-fit"></a></dt>
<dd><p>If <code>track_fit = TRUE</code>, <code>trace</code>
is also returned containing detailed information about the
estimates at each iteration of the IBSS fitting procedure.</p></dd>


<dt id="arg-residual-variance-lowerbound">residual_variance_lowerbound<a class="anchor" aria-label="anchor" href="#arg-residual-variance-lowerbound"></a></dt>
<dd><p>Lower limit on the estimated
residual variance. It is only relevant when
<code>estimate_residual_variance = TRUE</code>.</p></dd>


<dt id="arg-refine">refine<a class="anchor" aria-label="anchor" href="#arg-refine"></a></dt>
<dd><p>If <code>refine = TRUE</code>, then an additional
iterative refinement procedure is used, after the IBSS algorithm,
to check and escape from local optima (see details).</p></dd>


<dt id="arg-n-purity">n_purity<a class="anchor" aria-label="anchor" href="#arg-n-purity"></a></dt>
<dd><p>Passed as argument <code>n_purity</code> to
<code><a href="susie_get_methods.html">susie_get_cs</a></code>.</p></dd>


<dt id="arg-alpha-">alpha0<a class="anchor" aria-label="anchor" href="#arg-alpha-"></a></dt>
<dd><p>Numerical parameter for the NIG prior when using
<code>estimate_residual_method = "Servin_Stephens"</code>.</p></dd>


<dt id="arg-beta-">beta0<a class="anchor" aria-label="anchor" href="#arg-beta-"></a></dt>
<dd><p>Numerical parameter for the NIG prior when using
<code>estimate_residual_method = "Servin_Stephens"</code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A <code>"susie"</code> object with some or all of the following elements:</p>
<dl><dt>alpha</dt>
<dd><p>An L by p matrix of posterior inclusion probabilities.</p></dd>


<dt>mu</dt>
<dd><p>An L by p matrix of posterior means, conditional on inclusion.</p></dd>


<dt>mu2</dt>
<dd><p>An L by p matrix of posterior second moments, conditional on
  inclusion.</p></dd>


<dt>Xr</dt>
<dd><p>A vector of length n, equal to <code>X %*% colSums(alpha * mu)</code>.</p></dd>


<dt>lbf</dt>
<dd><p>Log-Bayes Factor for each single effect.</p></dd>


<dt>lbf_variable</dt>
<dd><p>Log-Bayes Factor for each variable and single effect.</p></dd>


<dt>intercept</dt>
<dd><p>Intercept (fixed or estimated).</p></dd>


<dt>sigma2</dt>
<dd><p>Residual variance (fixed or estimated).</p></dd>


<dt>V</dt>
<dd><p>Prior variance of the non-zero elements of b.</p></dd>


<dt>elbo</dt>
<dd><p>The variational lower bound (or ELBO) achieved at each iteration.</p></dd>


<dt>fitted</dt>
<dd><p>Vector of length n containing the fitted values.</p></dd>


<dt>sets</dt>
<dd><p>Credible sets estimated from model fit.</p></dd>


<dt>pip</dt>
<dd><p>A vector of length p giving the marginal posterior inclusion
  probabilities.</p></dd>


<dt>z</dt>
<dd><p>A vector of univariate z-scores.</p></dd>


<dt>niter</dt>
<dd><p>Number of IBSS iterations performed.</p></dd>


<dt>converged</dt>
<dd><p><code>TRUE</code> or <code>FALSE</code> indicating whether
  the IBSS converged to a solution within the chosen tolerance
  level.</p></dd>


<dt>theta</dt>
<dd><p>If <code>unmappable_effects = "inf"</code> or
  <code>unmappable_effects = "ash"</code>, then <code>theta</code> is a p-vector of posterior
  means for the unmappable effects.</p></dd>


<dt>tau2</dt>
<dd><p>If <code>unmappable_effects = "inf"</code> or
  <code>unmappable_effects = "ash"</code>, then <code>tau2</code> is the unmappable variance.</p></dd>

</dl></div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Gao Wang, Yuxin Zou, Alexander McCreight, Kaiqian Zhang, William R.P. Denault, Peter Carbonetto, Matthew Stephens.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

