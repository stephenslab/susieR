% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/susie.R, R/susie_rss.R, R/susie_ss.R,
%   R/susie_zzz_auto.R
\name{susie}
\alias{susie}
\alias{susie_rss}
\alias{susie_suff_stat}
\alias{susie_auto}
\title{Sum of Single Effects (SuSiE) Regression}
\usage{
susie(
  X,
  Y,
  L = min(10, ncol(X)),
  scaled_prior_variance = 0.2,
  residual_variance = NULL,
  prior_weights = NULL,
  null_weight = NULL,
  standardize = TRUE,
  intercept = TRUE,
  estimate_residual_variance = TRUE,
  estimate_prior_variance = TRUE,
  estimate_prior_method = c("optim", "EM", "simple"),
  check_null_threshold = 0,
  prior_tol = 1e-09,
  residual_variance_upperbound = Inf,
  s_init = NULL,
  coverage = 0.95,
  min_abs_corr = 0.5,
  compute_univariate_zscore = FALSE,
  na.rm = FALSE,
  max_iter = 100,
  tol = 0.001,
  verbose = FALSE,
  track_fit = FALSE
)

susie_rss(
  z,
  R,
  maf = NULL,
  maf_thresh = 0,
  z_ld_weight = 0,
  L = 10,
  prior_variance = 50,
  residual_variance = NULL,
  r_tol = 1e-08,
  prior_weights = NULL,
  null_weight = NULL,
  estimate_residual_variance = TRUE,
  estimate_prior_variance = TRUE,
  estimate_prior_method = c("optim", "EM", "simple"),
  check_null_threshold = 0,
  prior_tol = 1e-09,
  max_iter = 100,
  s_init = list(),
  intercept_value = 0,
  coverage = 0.95,
  min_abs_corr = 0.5,
  tol = 0.001,
  verbose = FALSE,
  track_fit = FALSE,
  check_R = TRUE,
  check_z = TRUE
)

susie_suff_stat(
  bhat,
  shat,
  R,
  n,
  var_y,
  XtX,
  Xty,
  yty,
  maf = NULL,
  maf_thresh = 0,
  L = 10,
  scaled_prior_variance = 0.2,
  residual_variance = NULL,
  estimate_residual_variance = TRUE,
  estimate_prior_variance = TRUE,
  estimate_prior_method = c("optim", "EM", "simple"),
  check_null_threshold = 0,
  prior_tol = 1e-09,
  r_tol = 1e-08,
  prior_weights = NULL,
  null_weight = NULL,
  standardize = TRUE,
  max_iter = 100,
  s_init = NULL,
  intercept_value = 0,
  coverage = 0.95,
  min_abs_corr = 0.5,
  tol = 0.001,
  verbose = FALSE,
  track_fit = FALSE,
  check_input = FALSE
)

susie_auto(
  X,
  Y,
  L_init = 1,
  L_max = 512,
  verbose = FALSE,
  init_tol = 1,
  standardize = TRUE,
  intercept = TRUE,
  max_iter = 100,
  tol = 0.01,
  ...
)
}
\arguments{
\item{X}{An n by p matrix of covariates.}

\item{Y}{The observed responses, a vector of length n.}

\item{L}{Number of components (nonzero coefficients) in the susie
regression model. If L is larger than the number of covariates, p,
L is set to p.}

\item{scaled_prior_variance}{The scaled prior variance. This is
either a scalar or a vector of length \code{L}. The prior variance
of each non-zero element of b is set to \code{var(Y) *
scaled_prior_variance}. If \code{estimate_prior_variance = TRUE},
this provides initial estimates of the prior variances.}

\item{residual_variance}{Variance of the residual. If
\code{estimate_residual_variance = TRUE}, this value provides the
initial estimate of the residual variance. By default, it is
\code{var(Y)}.}

\item{prior_weights}{A vector of length p, in which each entry
gives the prior probability that corresponding column of X has a
nonzero effect on the outcome, Y.}

\item{null_weight}{Prior probability of no effect (a number between
0 and 1, and cannot be exactly 1).}

\item{standardize}{If \code{standardize = TRUE}, standardize the
columns of X (or XtX and Xty) to unit variance prior to
fitting. Note that \code{scaled_prior_variance} specifies the prior
on the coefficients of X \emph{after} standardization (if it is
performed). If you do not standardize, you may need to think more
carefully about specifying \code{scaled_prior_variance}. Whatever
your choice, the coefficients returned by \code{coef} are given for
\code{X} on the original input scale. Any column of \code{X} that
has zero variance is not standardized.}

\item{intercept}{If \code{intercept = TRUE}, the intercept is
fitted; it \code{intercept = FALSE}, the intercept is set to
zero. Setting \code{intercept = FALSE} is generally not
recommended.}

\item{estimate_residual_variance}{If
\code{estimate_residual_variance = TRUE}, the residual variance is
estimated, using \code{residual_variance} as an initial value. If
\code{estimate_residual_variance = FALSE}, the residual variance is
fixed to the value supplied by \code{residual_variance}.}

\item{estimate_prior_variance}{If \code{estimate_prior_variance =
TRUE}, the prior variance is estimated (this is a separate
parameter for each of the L effects). If provided,
\code{scaled_prior_variance} is then used as an initial value for
the optimization. When \code{estimate_prior_variance = FALSE}, the
prior variance for each of the L effects is determined by the
value supplied to \code{scaled_prior_variance}.}

\item{estimate_prior_method}{The method used for estimating prior
variance. When \code{estimate_prior_method = "simple"} is used, the
likelihood at the specified prior variance is compared to the
likelihood at a variance of zero, and the setting with the larger
likelihood is retained.}

\item{check_null_threshold}{When the prior variance is estimated,
compare the estimate with the null, and set the prior variance to
zero unless the log-likelihood using the estimate is larger by this
threshold amount. For example, if you set
\code{check_null_threshold = 0.1}, this will "nudge" the estimate
towards zero when the difference in log-likelihoods is small. A
note of caution that setting this to a value greater than zero may
lead the IBSS fitting procedure to occasionally decrease the ELBO.}

\item{prior_tol}{When the prior variance is estimated, compare the
estimated value to \code{prior_tol} at the end of the computation,
and exclude a single effect from PIP computation if the estimated
prior variance is smaller than this tolerance value.}

\item{residual_variance_upperbound}{Upper limit on the estimated
residual variance. It is only relevant when
\code{estimate_residual_variance = TRUE}.}

\item{s_init}{A previous susie fit with which to initialize.}

\item{coverage}{A number between 0 and 1 specifying the
\dQuote{coverage} of the estimated confidence sets.}

\item{min_abs_corr}{Minimum absolute correlation allowed in a
credible set. The default, 0.5, corresponds to a squared
correlation of 0.25, which is a commonly used threshold for
genotype data in genetic studies.}

\item{compute_univariate_zscore}{If \code{compute_univariate_zscore
= TRUE}, the univariate regression z-scores are outputted for each
variable.}

\item{na.rm}{Drop any missing values in Y from both X and Y.}

\item{max_iter}{Maximum number of IBSS iterations to perform.}

\item{tol}{A small, non-negative number specifying the convergence
tolerance for the IBSS fitting procedure. The fitting procedure
will halt when the difference in the variational lower bound, or
\dQuote{ELBO} (the objective function to be maximized), is
less than \code{tol}.}

\item{verbose}{If \code{verbose = TRUE}, the algorithm's progress,
and a summary of the optimization settings, are printed to the
console.}

\item{track_fit}{If \code{track_fit = TRUE}, \code{trace}
is also returned containing detailed information about the
estimates at each iteration of the IBSS fitting procedure.}

\item{z}{A p-vector of z scores.}

\item{R}{A p by p symmetric, positive semidefinite matrix. It
can be \eqn{X'X}, the covariance matrix \eqn{X'X/(n-1)}, or a
correlation matrix. It should be estimated from the same samples
used to compute \code{bhat} and \code{shat}. Using an out-of-sample
matrix may produce unreliable results.}

\item{maf}{Minor allele frequency; to be used along with
\code{maf_thresh} to filter input summary statistics.}

\item{maf_thresh}{Variants having a minor allele frequency smaller
than this threshold are not used.}

\item{z_ld_weight}{The weights assigned to the z scores in the LD
matrix. The LD matrix used in the model is \code{cov2cor((1-w)*R +
w*tcrossprod(z))}, where \code{w = z_ld_weight}. We recommend
setting \code{z_ld_weight} as 1/n, where n is the number of
samples.}

\item{prior_variance}{The prior variance. It is either a scalar or
a vector of length L.}

\item{r_tol}{Tolerance level for eigenvalue check of positive
semidefinite matrix of R.}

\item{intercept_value}{The intercept. (The intercept cannot be
estimated from centered summary data.) This setting will be used by
\code{coef} to assign an intercept value, mainly for consistency
with \code{susie}. Set to \code{NULL} if you want \code{coef} not
to include an intercept term (and so only a p-vector is returned).}

\item{check_R}{If \code{check_R = TRUE}, check that \code{R} is
positive semidefinite.}

\item{check_z}{If \code{check_z = TRUE}, check that \code{z} is in
space spanned by the non-zero eigenvectors of \code{R}.}

\item{bhat}{A p-vector of estimated effects.}

\item{shat}{A p-vector of standard errors.}

\item{n}{The sample size.}

\item{var_y}{The sample variance of y, defined as \eqn{y'y/(n-1)}.
When the sample variance cannot be provided, the coefficients
(returned from \code{coef}) are computed on the "standardized" X, y
scale.}

\item{XtX}{A p by p matrix \eqn{X'X} in which the columns of X
are centered to have mean zero.}

\item{Xty}{A p-vector \eqn{X'y} in which y and the columns of X are
centered to have mean zero.}

\item{yty}{A scalar \eqn{y'y} in which y is centered to have mean
zero.}

\item{check_input}{If \code{check_input = TRUE},
\code{susie_suff_stat} performs additional checks on \code{XtX} and
\code{Xty}. The checks are: (1) check that \code{XtX} is positive
semidefinite; (2) check that \code{Xty} is in the space spanned by
the non-zero eigenvectors of \code{XtX}.}

\item{L_init}{The initial value of L.}

\item{L_max}{The largest value of L to consider.}

\item{init_tol}{The tolerance to passed to \code{susie} during
early runs (set large to shorten the initial runs).}

\item{\dots}{Additional arguments passed to \code{\link{susie}}.}
}
\value{
A \code{"susie"} object with some or all of the following
  elements:

\item{alpha}{An L by p matrix of posterior inclusion probabilites.}

\item{mu}{An L by p matrix of posterior means, conditional on
  inclusion.}

\item{mu2}{An L by p matrix of posterior second moments,
  conditional on inclusion.}

\item{Xr}{A vector of length n, equal to \code{X \%*\% colSums(alpha
  * mu)}.}

\item{lbf}{log-Bayes Factor for each single effect.}

\item{lbf_variable}{log-Bayes Factor for each variable and single effect.}

\item{intercept}{Intercept (fixed or estimated).}

\item{sigma2}{Residual variance (fixed or estimated).}

\item{V}{Prior variance of the non-zero elements of b, equal to
  \code{scaled_prior_variance * var(Y)}.}

\item{elbo}{The value of the variational lower bound, or
  \dQuote{ELBO} (objective function to be maximized), achieved at
  each iteration of the IBSS fitting procedure.}

\item{fitted}{Vector of length n containing the fitted values of
  the outcome.}

\item{sets}{Credible sets estimated from model fit; see
  \code{\link{susie_get_cs}} for details.}

\item{pip}{A vector of length p giving the (marginal) posterior
  inclusion probabilities for all p covariates.}

\item{z}{A vector of univariate z-scores.}

\item{niter}{Number of IBSS iterations that were performed.}

\item{converged}{\code{TRUE} or \code{FALSE} indicating whether
  the IBSS converged to a solution within the chosen tolerance
  level.}

\code{susie_suff_stat} returns also outputs:

\item{XtXr}{A p-vector of \code{t(X)} times the fitted values,
  \code{X \%*\% colSums(alpha*mu)}.}

\code{susie_rss} also outputs:

\item{Rr}{An p-vector of \code{t(X)} times fitted values, \code{X
  \%*\% colSums(alpha*mu)}.}
}
\description{
Performs Bayesian multiple linear regression of Y on
  X; that is, this function fits the regression model \eqn{Y = \sum_l
  X b_{l=1}^L + e}, where elements of e are \emph{i.i.d.} normal with
  zero mean and variance \code{residual_variance}, and
  \eqn{\sum_{l=1}^L b_l} is a vector of length p representing the
  effects to be estimated. The \dQuote{susie assumption} is that each
  \eqn{b_l} has exactly one non-zero element. The prior on the
  non-zero element is normal with zero mean and variance \code{var(Y)
  * scaled_prior_variance}. The model is fitted using the
  \dQuote{Iterative Bayesian Stepwise Selection} (IBSS) algorithm.
  See also \code{\link{susie_trendfilter}} for applying susie to
  non-parametric regression, particularly changepoint problems.
}
\details{
\code{susie_suff_stat} performs sum of single-effect
linear regression with summary statistics. The required summary
data are either: \code{bhat}, \code{shat}, the p by p symmetric,
positive semidefinite correlation (or covariance) matrix \code{R},
the sample size \code{n}, and the variance of y; or the p by p
matrix \eqn{X'X}, the p-vector \eqn{X'y}, the sum of squares
\eqn{y'y}, and the sample size \code{n}. The summary statistics
should come from the same individuals. Both the columns of X and
the vector y should be centered to have mean zero before computing
these summary statistics; you may also want to scale each column of
X and y to have variance 1 (see examples).

\code{susie_rss} performs sum of single-effect linear regression
with z scores; all posterior calculations are for z-scores. This
function fits the regression model \eqn{z = \sum_l R*b_l + e},
where e is \eqn{N(0,residual_var*R)} and \eqn{\sum_l b_l} is a
p-vector of effects to be estimated. The required summary data are
the p by p correlation matrix, \code{R}, and the p-vector
\code{z}. The summary stats should come from the same individuals
(samples).




susie_auto is an attempt to automate reliable running of susie even
on hard problems. It implements a three-stage strategy for each L:
first, fit susie with very small residual error; next, estimate
residual error; finally, estimate the prior variance. If the last
step estimates some prior variances to be zero, stop. Otherwise,
double L, and repeat. Initial runs are performed with relaxed
tolerance; the final run is performed using the default susie
tolerance.
}
\examples{

# susie example.
set.seed(1)
n = 1000
p = 1000
beta = rep(0,p)
beta[1:4] = 1
X = matrix(rnorm(n*p),nrow = n,ncol = p)
X = scale(X,center = TRUE,scale = TRUE)
y = drop(X \%*\% beta + rnorm(n))
res1 = susie(X,y,L = 10)
plot(beta,coef(res1)[-1])
abline(a = 0,b = 1,col = "skyblue",lty = "dashed")
plot(y,predict(res1))
abline(a = 0,b = 1,col = "skyblue",lty = "dashed")

# susie_suff_stat example.
input_ss = compute_ss(X,y,standardize = TRUE)
res2 = with(input_ss,
            susie_suff_stat(XtX = XtX,Xty = Xty,yty = yty,n = n,L = 10))
plot(coef(res1)[-1],coef(res2)[-1])
abline(a = 0,b = 1,col = "skyblue",lty = "dashed")

# susie_rss example.
ss   <- susieR:::univariate_regression(X,y)
R    <- with(input_ss,cov2cor(XtX))
zhat <- with(ss,betahat/sebetahat)
res3 <- susie_rss(zhat,R,L = 10)
plot(coef(res1)[-1]/ss$sebetahat,coef(res3)[-1])
abline(a = 0,b = 1,col = "skyblue",lty = "dashed")

# susie_auto example.

}
\references{
G. Wang, A. Sarkar, P. Carbonetto and M. Stephens (2020). A simple
  new approach to variable selection in regression, with application
  to genetic fine-mapping. \emph{Journal of the Royal Statistical
  Society, Series B} \url{https://doi.org/10.1101/501114}.
}
